# DSA-Model

Model base on Show, Attend and Tell: Neural Image Caption Generation with Visual Attentiont, Soft Attention.
Mscoco model base on coldmanck implementation (https://github.com/coldmanck/show-attend-and-tell)
Flickr 8k & 30K base on fuqichen implementation (https://github.com/fuqichen1998/eecs442-final-project-show-and-tell)
- CNN Layer Model: VGG16 (default)
- RNN Layer Model: LSTM (default)
- Datasets: MS-COCO, Flickr8k & Flickr30k
- Scoring: BLEU_1, BLEU_2, BLEU_3, BLEU_4, METEOR, ROUGE_L, CIDEr

## Requirements
- **DATA zip file: https://drive.google.com/file/d/16jNwTdwtFXoW_gsxH87TntWh6ICcFzIj/view?usp=sharing**
- Check each implementation README.md of each dataset

### Installation 
- Check each implementation README.md of each dataset

## References

* Kevin Xu, et al. Show, Attend and Tell: Neural Image Caption Generation with Visual Attention (https://arxiv.org/pdf/1502.03044.pdf)
* fuqichen EECS442 Final Project Winter 2019 repo (https://github.com/fuqichen1998/eecs442-final-project-show-and-tell)
* coldmanck Python 3 Version of Show, Attend and Tell using Tensorflow repo (https://github.com/coldmanck/show-attend-and-tell)
* Lin, Tsung-Yi Microsoft COCO Caption Evaluation repo (https://github.com/tylin/coco-caption)
